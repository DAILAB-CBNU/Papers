텍스트 채우기와 적대 신경망을 이용한 개체명 인식 데이터 확장
=========================================================

- 저자: 박천용, 이공주
- [텍스트_채우기와_적대_신경망을_이용한_개체명_인식_데이터_확장.pdf](/uploads/e09ede6cf9ca8813cf1b7dee4c8be9a7/텍스트_채우기와_적대_신경망을_이용한_개체명_인식_데이터_확장.pdf)

서론
---------

- 학습 데이터 부족 문제를 해결하기 위한 데이터 확장 방식(데이터를 변형하여 새로운 데이터를 만듦)
- 텍스트 문장 데이터 확장: 기계 번역으로 역번역, 오토 인코더, 샘플링 알고리즘, 문장 내 단어 변형
- 개체명 인식 데이터의 레이블링 변경 없이 텍스트 채우기로 데이터 확장 시도

관련 연구
---------------

- 생성적 적대 신경망(GAN)
  - 잠재 벡터로부터 이미지나 문자열을 생성하는 생성기와 실제 데이터와 생성기가 생성한 데이터를 구별하는 판별기로 이루어져 있음
  - GAN의 학습 불안정성을 Wassersteion 거리를 이용하여 보완하는 WGAN의 판별기를 이용하여 모델이 자연스러운 문장 생성을 할 수 있도록 하였음

- 텍스트 채우기
  - 트랜스포머 디코더 기반 모델의 BERT에 WGAN의 판별기를 추가하여 텍스트 채우기를 통한 테이터 확장을 함

텍스트 채우기 모델을 이용한 개체명 인식 데이터 확장(Method)
--------------------------------------------------------

- 개체명 인식 데이터 확장을 위한 템플릿
  - 개체명을 제외하고 남은 부분 문장 중에서 가장 긴 부분 문장을 선택하고 마스킹 비율만큼 마스킹함
  - ![image](https://user-images.githubusercontent.com/49019184/221734969-715103e1-dfd1-490b-9720-952066c26283.png)

- 트랜스포머와 WGAN 기반의 텍스트 채우기 모델
  - TI-TDWG(Text Infilling using Transformer's Decoder and WGAN)
  - ![image](https://user-images.githubusercontent.com/49019184/221734979-4b39af98-be44-466f-985b-ef9a7880d199.png)
  - 템플릿을 입력받고(Masking Function) 마스킹 위치의 부분 문장을 만들어내는 생성기와(Generator) 생성된 부분 문장(Generated sub-sentence)과 정답 부분 문장(Real sub-sentence)을 입력받아 Wasserstein 거리를 계산하는 판별기로(Discriminator) 구성됨
  - 생성기
    - KorBERT의 인코더(형태소 단위의 토큰)와 트랜스포머 디코더로 구성됨
    - ![image](https://user-images.githubusercontent.com/49019184/221735003-f4855cfa-ff5f-48bd-9b4c-ed60277cb668.png)
    - 인코더의 입력은 템플릿 문자열($t$)이고 디코더의 입력은 정답 부분 문장과 인코더의 출력(템플릿 임베딩)임
    - 생성기의 손실 함수: $L_{gen} = -\sum_i^V \log{Dec(BERT(t),a))}$

  - 판별기
    - 정답과 생성기의 출력의 분포 거리(Wasserstein 거리)를 줄이도록 학습함
    - ![image](https://user-images.githubusercontent.com/49019184/221735019-946771dd-817e-46b7-8235-5e91a7e53c34.png)
    - 손실 함수: $L_{dis} = D(t,a)-D(t,g)+\lambda(\Vert\nabla_{\hat{g}}D(\hat{g})\Vert)^2$
    - 무작위 값인 $\epsilon$ 비율로 계산한 보간 값: $\hat{g} = \epsilon a+(1-\epsilon)g$

  - 모델 학습과정
    - 생성기와 판별기를 번갈아 가며 학습함
    - 모델의 학습 알고리즘 ![image](https://user-images.githubusercontent.com/49019184/221735041-e9f1c242-a8ac-4be0-b1ab-828d39d914c8.png)


실험 및 평가
----------------

- 데이터셋: ETRI 언어처리 데이터, 2016 국어 정보 처리 경진대회 데이터, 해양대학교 개체명 인식 데이터 + ETRI 언어 분석기로 분석한 인터넷 신문기사 데이터
- 전처리: 문장 길이 제한(형태소 토큰 기준 10 토큰 이하의 문장), 개체명이 절반 이상 나타난 문장 제외
- ![image](https://user-images.githubusercontent.com/49019184/221735062-87242e98-c86e-464c-b10e-5533e600ef6a.png)
- LSTM기반의 시퀀스 투 시퀀스 모델과 트랜스포머 디코더 기반의 Text infilling 모델과 성능 비교

- 정량적 평가
  - 원문과 생성된 문장의 유사도를 평가하는 BLEU, 얼마나 다양한 표현이 생성되었는지 계산하는 self-BLEU, 문장이 자연스럽게 만들어졌는지를 평가하는 문장 복잡도(perplexity)
  - ![image](https://user-images.githubusercontent.com/49019184/221735075-c70de565-d132-4b6b-8e57-878b744f8692.png)
  - 마스킹 비율에 따른 평가, Template의 BLEU가 낮음 == 생성 난이도가 어려움
  - ![image](https://user-images.githubusercontent.com/49019184/221735088-30ba8edc-9c60-4f68-84dd-54d1f323d32b.png)
  - 다른 모델과 비교 평가, 제안된 모델이 다른 모델에 비해 다양하고 자연스러운 부분 문장을 생성하였음
  - ![image](https://user-images.githubusercontent.com/49019184/221735103-9e78c615-258c-468f-b2e9-47d4fc3af520.png)
  - Template과 Generated의 임베딩 거리, 의미적 유사성이 있는 문장을 생성하는 것으로 확인되었음
  - ![image](https://user-images.githubusercontent.com/49019184/221735117-5484a2a5-21c9-48ce-8475-890285f9cf14.png)
  - BERT와 WGAN을 제거하면서 성능 평가, BERT로 인해 BLEU가 높아짐(템플릿의 문맥정보를 더 잘 반영함), WGAN으로 인해 문장 복잡도가 낮아짐(자연스러운 문장을 생성함)

- 정성적 평가
  - 얼마나 문법적이고 일관적인 내용을 갖는지 3명의 평가자에게 순위를 부여하도록 함
  - ![image](https://user-images.githubusercontent.com/49019184/221735129-4331094c-6669-493b-9f99-907f28b9b960.png)
  - ![image](https://user-images.githubusercontent.com/49019184/221735143-db2efb71-16be-4c4b-bd4c-391f2beb19c3.png)
  - 정성적 평가 결과, 다른 모델에 비해 높은 순위를 보여줌
  - ![image](https://user-images.githubusercontent.com/49019184/221735159-6f7e5840-4ef3-44b9-87b2-a6fbadab26b9.png)
  - 데이터 확장을 한 후의 개체명 인식의 f1 score, 가장 높은 점수를 얻었음

결론
------

- 트랜스포머 디코더 기반의 텍스트 채우기 모델로 개체명 인식 데이터를 확장하는 TI-TDWG 제안
- BERT의 인코더와 WGAN의 판별기를 적용하여 다양하고 자연스러운 부분 문장 생성이 가능해졌음
- 사람이 평가하기에도 자연스럽고 템플릿과 연관성 있는 문장을 생성하였음
- 확장한 데이터로 인해 개체명 인식의 성능이 향상된 것을 확인하였음
