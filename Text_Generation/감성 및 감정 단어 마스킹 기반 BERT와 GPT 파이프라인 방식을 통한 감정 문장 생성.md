#### 논문 - 감성 및 감정 단어 마스킹 기반 BERT와 GPT 파이프라인 방식을 통한 감정 문장 생성
- 저자: 이원민, 온병원
- 한국정보기술학회 2021
- [논문 링크](http://ki-it.com/xml/30455/30455.pdf)
------------------------------------------------------------------
- **목적**
  - 챗봇과의 대화에서 사용자의 감정을 파악하지 못하면 엉뚱한 대답과 몰입감을 방해한다는 문제점이 있음
  - 본 논문에서는 BERT를 활용하여 대화 말뭉치를 자동으로 감정 레이블링하고 GPT로 응답 문장을 생성하는 BERT+GPT 파이프라인 방안 제안  
    - 기존 연구의 장기 기억과 학습 속도 문제점을 개선하고 감정이 반영된 응답 문장을 생성하기 위해 사용함  
  - 크게 두 가지 단계로 구성된 방안을 제안함   
    ![image](https://user-images.githubusercontent.com/49019292/223056854-c94ae009-6992-4c7f-ae3f-9f42b03d63d2.png)   
    - 1번째 단계는 감정을 레이블링하는 단계로, 행복, 불안, 슬픔, 당황, 상처, 분노, 중립 중 하나로 분류하고 BERT 활용
    - 두번째 단계는 감정이 반영된 응답 문장을 생성하는 단계로, KoGPT2 사용 
      - 생성된 문장은 긍정 또는 부정으로 분리됨
  - 생성된 문장의 평가
    - 첫번째 평가 방식은 정성적 평가 방식으로 무작위로 추출한 300개의 대화 말뭉치를 4번에 걸쳐 평가 진행 
      - 평가자 3명이 생성된 응답 문장에 대해 감정의 일과넛ㅇ, 상황의 적절성을 1~5점으로 평가
    - 두번째 평가 방식은 무작위로 추출한 1200개의 대호 말무치에 대해 정량적 평가 진행
      - 사영자의 입력값과 생성되는 응답 문장의 감정이 일치하는지 BERT를 통해 정확도 평가   
-------------------------------------------------------------------------
- **방법**
  - BERT와 KoGPT2를 활용하여 대화 말뭉치의 감정을 분류하고 응답 문장을 생성하는 방안 설명
    - BERT 사전 학습 단계에서는감성 단어 5523개와 감정 단어 428개를 사용하여 마스킹 진행
    - 감정 단어는 서울대 심리학과에서 현대 한국어 어휘 빈다 자료집을 활용해 일상생활에서 빈도 높게 사용하는 감정 표현 단어만 추출한 목록
    - KNU 감성 사전의 단어(감성 사전)도 추가하여 사용 
    - 감성 및 감정 마스킹을 위한 단어 추출 방안     
      - 아래 Algorithm1은 BERT 사전학습 단계에서 단어 마스킹을 윟나 감성 및 감정 단어 추출 알고리즘   
        ![image](https://user-images.githubusercontent.com/49019292/223056899-d6f8e891-caed-41dc-a6fe-b37b08781a60.png)    
        - 토큰 중 감성 및 감정이 있는 토큰을 KNU 감성 사전과 감정 사전과의 매칭을 통해 추출됨
        - 추출한 단어를 BERT에서 사용하는 토큰 형식에 맞게 감성 및 감정 단어, ##감성 및 감정 단어 형식으로 변경    
    - BERT 사전 학습
      - 아래 그림 2의 Pre-training과 같이 입력 문장이 2개 들어감   
        ![image](https://user-images.githubusercontent.com/49019292/223056955-a87c3249-bb84-4655-8f01-d79f58d3b492.png)   
        - 입력 문장에서 감성 및 감정 단어를 마스킹하고 없을 때는 15%를 무작위로 샘필릉하여 마스킹함
    - BERT의 파인튜닝
      - 그림 2의 Fine-tuning과 같이 입력값은 한 문장씩 들어감
        - 문장은 단일 및 대화에 사용된 7개의 감정 중 하나로 레이블링 된 문장 약 23만개를 사용
        - 학습에 사용된 각 감정에 대한 예시 문장   
          ![image](https://user-images.githubusercontent.com/49019292/223056982-fb8476a8-7c48-4aaf-bd06-04ba456bc9db.png)    
    - KoGPT2 모델을 활용한 문장 생성 방안
      - KoGPT2 모델은 사전학습과 파인튜닝 단계로 구성됨
      - 사전 학습은 따로 진행하지 않고 기존 모델 사용
      - 파인 튜닝을 위한 학습 데이터는 대화에 사용된 문장 15만개를 위에서 제시한 방안에 적용해 감정을 레이블링 하여 사용함
---------------------------------------------
- **실험 및 결과**
  - 첫번째 단계의 BERT의 사전 학습 과정에서는 위키 백과, KorQuAD, 네이버 영화 리뷰 등 총 5413400개이 한국어 문장을 사용하여 실험 진행
    - BERT의 파인튜닝은 AI Hub에서 수집한 감정 레이블링 된 한국어 대화 말뭉치와 감정 레이블링된 한국어 단일 문장 말뭉치, 미디어젠에서 수집한 감정 레이블링 된 한국어 대화 말뭉치 사용  
      - 총 238600개의 문장 사용
      - 8:2 비율로 분할하여 각각 학습과 테스트에 사용
    - 모델 정확도는 기존의 무작위로 샘플링된 단어 마스킹 기반의 BERT model과 제안한 감성 및 감정 단어 마스킹 기반의 BERT 모델 각각의 감정 레이블링 결과를 정답 값과의 비교를 통해 계산함 
      - 정확도 계산식   
        ![image](https://user-images.githubusercontent.com/49019292/223057012-61f4f27d-573f-434a-be60-d3b02788b644.png)   
  - 파인튜닝 단계에서는 AI Hub와 미디어젠에서 제공한 말뭉치 총 150390개의 문장을 사용함
    - 문장은 한 대화에서 2개씩 결합해 하나의 쌍으로 재구성  
    - 이를 Baseline method와 propsed mothod로 각각 감정 레이블링 진행
    - 레이블링 된 데이터를 8:2로 분할하여 각각 학습과 테스트에 사용
  - KoGPT2의 평가는 2가지 방식으로 진행
    - 정성적 평가 방식
      - 평가 항목은 생성되는 문장의 감정 일관성, 상황에 적절한 문장인지에 대한 자연스러움 2가지를 평가함
      - 테스트 데이터 중 300개의 문장 쌍을 4번에 걸쳐 무작위로 추출하여 평가자 3명이 평가함
      - 일관성과 자연스러움을 각각 1~5점으로 평가함  
      - 사용자들의 평가 결과에 대한 신뢰성 확보를 위해 Kendall 상관계수 측정도 진행(평가 사이의 평가 일치 정도를 평가함)
    - 정량적 평가 방식
      - 2 문장이 생성되었을 때 BERT를 사용해 두 문장이 같은 감정인지 정확도를 측정
    - 표 2는 감정 분류와 문장 생성에서 사용된 BERT와 KoGPT2 모델의 하이퍼 파라미터임   
        ![image](https://user-images.githubusercontent.com/49019292/223057049-fe8ccbd4-5050-4cfb-9ace-a2b10c8bf71a.png)    
    - 표 3은 실험에서 사용한 컴퓨터 사양   
      ![image](https://user-images.githubusercontent.com/49019292/223057070-c8a3176b-0c5a-4d1c-9d12-fb4ce10ba55c.png)   
  - 실험 결과
    - 그림 5는 표 2의 하이퍼 파라미터 값과 표 3의 컴퓨터 하드웨어 사양을 사용하여 Baseline method와 Proposed method 각각에 대한 문장의 감정분류 정확도를 사용해 계산하여 비교한 결과   
      ![image](https://user-images.githubusercontent.com/49019292/223057090-e3b5b69c-35b9-4e0f-ad55-ba8cdcfb3a2d.png)    
    - 그림 6은 KoGPT2를 통한 문장 생성에서 정석적 평가의 결과   
      ![image](https://user-images.githubusercontent.com/49019292/223057107-22624868-3872-4217-9449-0d3810f9fe54.png)   
    - 표 4는 그림 6에 대한 각 평가자의 평가 결과의 신뢰성 확보를 위해 Kendall 상관계수를 측정한 결과(값이 클수록 평가자들끼리 비슷한 평가를 했음을 의미함)   
      ![image](https://user-images.githubusercontent.com/49019292/223057127-1ca53b9a-43d2-48b7-b029-8f235c9fa2e3.png)    
    - 그림 7은 KoGPT2를 통해 생성된 문장을 정량적으로 평가한 결과   
      ![image](https://user-images.githubusercontent.com/49019292/223057147-dd2cb0be-b4d7-44d2-a2b7-575a7b6a9e0f.png)      
    - 표 5는 Baseline nethod와 Proposed method를 사용해 생성된 실제 응답 문장의 예시   
      ![image](https://user-images.githubusercontent.com/49019292/223057165-2909624f-5ec8-4db1-b15f-b056b016d0db.png)   
-----------------------------------------------------------
- **고찰**
  - 생성된 결과 예시가 너무 재밌다 ㅋㅋㅋㅋ
    - 중간이 기존 모델들만 조합해서 생성한 것이고, 세번째가 여기서 제안하는 감성과 감정에 마스킹하여 학습한 모델이 생성한 문장인데 기존 모델들만 사용한 문장은 약간 사회부적응자처럼 이야기하는 경향을 보이는 듯   
      ![image](https://user-images.githubusercontent.com/49019292/223057196-fb13ca2b-a025-4f83-83fd-6a46bd440383.png)     
      - 무작위로 마스킹하였기때문에 확실히 문장의 흐름(맥락 혹은 감정)을 읽는 능력이 떨어지긴 하는 것 같다.
      - 기존 모델은 주식이라는 단어에 초점을 맞춰서 보통의 지식으로 생성된 문장을 내보내지만, 여기서 제안하는 모델은 Human의 감정에 더 초점이 맞춰져있는 것 같다.
  - 재밌는 실험이고 결과도 흥미로웠다.
  - 근데 인간의 감정에 맞춰 응답을 생성하는데, 생각보다 의문형이 많은 것 같다.
    - 예시만 봐도, "괜히 결혼을 했나봐"라는 문장에, "왜 그렇게 생각하시나요?"라고 답한 점을 볼 수 있다.
    - 왜 응답의 질문 형태가 많이 생성될까?
    - Human의 대화에서 감정이나 감성을 쉽게 찾을 수 없을 때 질문형 응답을 만들어내는걸까??
    - 물론 자연스럽지 않은 문장은 아닌데 질문형 응답을 생성해내는 조건?이 궁금하다..!
