사전학습 언어모델 기반 트랜스포머를 활용한 의미유사도기반 자연어이해 의도파악 방법
==================================================================================

- 저자: 정상근, 서혜인, 김현지, 황태욱
- https://gitlab.com/bytecell/dailab/-/issues/60#note_1240172634

서론
------------

- 자연어이해(NLU)는 비정형 텍스트 형태의 데이터를 분석하여 의미틀이라는 정형 구조를 가진 정형 텍스트로 가공하는 기술임, FrameNet 방식이나 Slot-Filling 방식의 다양한 의미틀이 존재함
- Slot-Filling방식은 의도파악 기술과 영역 개체명 분석 기술로 이루어짐, 본 논문에서는 의도파악을 수행하는 새로운 방법을 제시함
- ![image](https://user-images.githubusercontent.com/49019184/213093798-a952d351-87d3-47f9-9a11-3bc174d279b8.png)
- 분류하는 방식이 아닌 유사도에 기반한 의도파악 방법을 제시함
- 입력 문장과 의미틀을 읽는 것을 학습시키고, 입력 문장의 의미벡터와 가장 가까운 훈련 문장을 벡터유사도로 찾은 후, 해당 훈련 문장의 의도태그를 입력 문장의 의도태그로서 부착함
- 기존 방법보다 높은 성능을 보이고 문장별 의도벡터를 구할 수 있기에 시각화가 가능해짐

의미벡터 학습 방법
---------------------------

![image](https://user-images.githubusercontent.com/49019184/213093807-207ee6c4-b53b-4220-b85b-7090cba8cef1.png)
- 트랜스포머 활용 문장 및 의미틀 읽기장치
  - 사전학습된 BERT 활용, 한국어 말뭉치는 BERT-base-multilingual-cased, 영어 말뭉치는 BERT-base-cased
  - 문장 읽기장치는 BERT tokenizer로 문장을 토큰으로 분리하고 토큰에 대한 임베딩 값을 계산하여 문장 의미벡터 $v_t$로 표시함
  - 의미틀은 의도태그, 개체태그 배열, 개체명 배열로 구성됨
  - 의미틀 읽기장치는 의도태그 읽기장치와 개체태그 및 개체명 읽기장치로 구성되고 의미틀을 다음과 같은 <의도태그>(개체태그=개체값, 개체태그=개체값, ...) 형태의 문장으로 가공함, 예)"weather.dust(day.p=오늘)" 최종 출력값은 의미틀 의미벡토 $v_s$로 표시함

- 학습 방법
  - 문장과 의미틀은 정의상 서로 같은 의미를 가져야 함, $L_{dist} = dist(v_t, v_s)$(거리비용, consine 거리함수 사용)
  - 의미틀 생성비용: 의미를 충분히 담고 있는 의미벡터라면 그로부터 실제 의미틀이 생성될 수 있어야 함, 의미틀 생성비용이 없을 경우, $v_t$와 $v_s$는 빠르게 0으로 수렴함(의미표현을 제대로 학습하지 못한 경우)
  - ![image](https://user-images.githubusercontent.com/49019184/213093821-282b7450-7374-4caf-873a-66757227ede2.png)
  - 의도태그 생성 $y_{intent} = w_I^{\prime}v + b_I$(선형투사 방식)
  - 개체태그배열 생성 $\overrightarrow{q_m} = R_G(v, \overrightarrow{q_{m-1}})$, $y_{slot}^m = W_S^{\prime} \overrightarrow{q_m} + b_s$(RNN)
  - 최종 목적손실함수 $L = L_{dist} + L_{content}$, 의미틀 생성비용 $L_{content} = \frac{1}{2}*L_{content}^{v_t} + L_{content}^{v_s}$, $L_{intent}^v$와 $L_{slot}^v$은 크로스 엔트로피로 계산
  

의미벡터 유사도를 이용한 의도파악
------------------------------------

![image](https://user-images.githubusercontent.com/49019184/213093840-bbfbc2d2-f779-4294-be84-c81f27fa51a9.png)
- 학습을 통해 얻은 문장 읽기장치 $R_T$를 이용하여 데이터의 모든 문장에 대한 의미벡터를 얻어냄
- 한 문장에 대한 의미벡터를 $R_T$를 활용하여 구하고 각 문장의 의미벡터와의 거리 값을 비교함
- 벡터의 거리가 가까운 문장에서 먼 순서대로 정렬하고 상위 K개 안에서 많이 나타난 의도태그를 최종 의도태그로 취함


실험
----------

- 한국어 기반 말뭉치: Weather, Navi   영어 기반 말뭉치: Atis, Snips
- ![image](https://user-images.githubusercontent.com/49019184/213093853-67930c4c-2161-48c6-8d6d-dd4f687e94fb.png)
- LSTM과 사전학습 되지 않은 BERT, KoBERT, KorBERT, RoBERTa, ALBERT 모델 사용
- ![image](https://user-images.githubusercontent.com/49019184/213093868-ac830c31-5ecd-4b33-ab7b-a098999b72fc.png)
- 의도태그 분포, Atis의 경우 불균형하게 분포되어 있음
- ![image](https://user-images.githubusercontent.com/49019184/213093888-d49930e5-d7e8-4e4d-b3e5-68241afddb27.png)
- 개체태그 분포, Atis의 경우 매우 긴꼬리를 갖고 있음
- ![image](https://user-images.githubusercontent.com/49019184/213093902-c82c67e9-3b77-482d-b3c2-15f11ae34760.png)
- 벡터 거리 상위 K개 안에 실제 정답 의도가 몇 개나 포함되었는지 측정
- ![image](https://user-images.githubusercontent.com/49019184/213093913-8b486a98-ba0b-4ecb-b236-b262fc1a03d7.png)
- 학습 속도 비교, BERT 기반 방법론이 더 낮은 손실과 동일 수준 손실에 도달하는 시점도 더 빠름
- ![image](https://user-images.githubusercontent.com/49019184/213093932-9ab5682c-1760-433f-bdf8-bd1664748ed5.png)
- 의도파악 예제
- ![image](https://user-images.githubusercontent.com/49019184/213093951-33db9887-a9b7-43ad-b339-98346adae327.png)
- 벡터 거리에 따른 예측 결과
- ![image](https://user-images.githubusercontent.com/49019184/213093962-ac4acca7-8c00-493f-b836-e3d9eb1b56ab.png)
- 계산된 벡터값을 UMAP방식으로 시각화


결론
----------

- 사전학습된 BERT 기반 문장유사도를 활용한 의도파악 기술 제안
- 사전학습 언어모델 기반 BERT를 활용해 방법론의 우수성 검증
- 말뭉치 내의 태그 간 불균형 문제 등이 강건한 의미벡터 공간을 구성하는데 악영향을 보임
